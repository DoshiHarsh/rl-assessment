{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import redis\n",
    "import grpc\n",
    "from seniority_grpc import SeniorityModel_pb2\n",
    "from seniority_grpc import SeniorityModel_pb2_grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "redis_client = redis.Redis(host='local-host',port=12252,password='xxxx')\n",
    "channel = grpc.insecure_channel('localhost:50051')\n",
    "stub = SeniorityModel_pb2_grpc.SeniorityModelStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl_file_from_s3(bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file from S3 and returns a list of job postings (dict).\n",
    "    \"\"\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    lines = obj['Body'].read().decode('utf-8').splitlines()\n",
    "    job_postings = [json.loads(line) for line in lines]\n",
    "    return job_postings\n",
    "\n",
    "def deduplicate_job_postings(job_postings):\n",
    "    \"\"\"\n",
    "    Deduplicates the job postings based on (company, title).\n",
    "    Returns a set of unique (company, title) tuples.\n",
    "    \"\"\"\n",
    "    unique_pairs = {(job['company'], job['title']) for job in job_postings}\n",
    "    return unique_pairs\n",
    "\n",
    "def check_cache(unique_pairs):\n",
    "    \"\"\"\n",
    "    Check Redis cache for (company, title) pairs.\n",
    "    Returns a dictionary with cache hits and cache misses.\n",
    "    \"\"\"\n",
    "    cache_hits = {}\n",
    "    cache_misses = {}\n",
    "    uuid_counter = 0\n",
    "    \n",
    "    for company, title in unique_pairs:\n",
    "        cache_key = f\"{company}:{title}\"\n",
    "        seniority = redis_client.get(cache_key)\n",
    "        \n",
    "        if seniority:\n",
    "            cache_hits[(company, title)] = int(seniority)\n",
    "        else:\n",
    "            cache_misses[uuid_counter]= (company, title)\n",
    "            uuid_counter+=1\n",
    "    \n",
    "    return cache_hits, cache_misses\n",
    "\n",
    "def grpc_infer_seniority(cache_misses):\n",
    "    \"\"\"\n",
    "    Calls the gRPC endpoint for all cache misses.\n",
    "    Returns a dictionary with seniority levels for the given (company, title) pairs.\n",
    "    \"\"\"\n",
    "    seniority_request_batch = SeniorityModel_pb2.SeniorityRequestBatch(\n",
    "        batch=[SeniorityModel_pb2.SeniorityRequest(uuid=uuid, company=value[0], title=value[1]) for uuid, value in cache_misses.items()]\n",
    "    )\n",
    "    \n",
    "    response = stub.InferSeniority(seniority_request_batch)\n",
    "    seniority_dict = {(cache_misses[resp.uuid][0], cache_misses[resp.uuid][1]): resp.seniority for resp in response.batch}\n",
    "    \n",
    "    return seniority_dict\n",
    "\n",
    "\n",
    "def update_cache(seniority_dict):\n",
    "    \"\"\"\n",
    "    Update Redis cache with the inferred seniority levels.\n",
    "    \"\"\"\n",
    "    for (company, title), seniority in seniority_dict.items():\n",
    "        cache_key = f\"{company}:{title}\"\n",
    "        redis_client.set(cache_key, seniority) \n",
    "\n",
    "def augment_job_postings(job_postings, cache_hits, grpc_results):\n",
    "    \"\"\"\n",
    "    Augment each job posting with the corresponding seniority level.\n",
    "    \"\"\"\n",
    "     # Combine cache hits and gRPC results\n",
    "    seniority_info = {**cache_hits, **grpc_results}\n",
    "    \n",
    "    for job in job_postings:\n",
    "        company_title_pair = (job['company'], job['title'])\n",
    "        job['seniority'] = seniority_info.get(company_title_pair, None)  # Assign seniority if available\n",
    "    \n",
    "    return job_postings\n",
    "\n",
    "def write_to_s3(bucket_name, file_key, augmented_data):\n",
    "    \"\"\"\n",
    "    Write augmented job postings to S3 as JSONL.\n",
    "    \"\"\"\n",
    "    output_lines = [json.dumps(job) for job in augmented_data]\n",
    "    output_body = \"\\n\".join(output_lines)\n",
    "    \n",
    "    s3.put_object(Bucket=bucket_name, Key=file_key, Body=output_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    AWS Lambda function triggered by S3 file uploads. Processes the newly uploaded files.\n",
    "    \"\"\"\n",
    "    # Extract bucket and file key from the event\n",
    "    for record in event['Records']:\n",
    "        bucket = record['s3']['bucket']['name']\n",
    "        file_key = record['s3']['object']['key']\n",
    "        \n",
    "        # Process the file\n",
    "        process_file(bucket, 'rl-data', file_key)\n",
    "\n",
    "\n",
    "def process_file(bucket_input, bucket_output, file_key):\n",
    "    # Step 1: Read the JSONL file\n",
    "    job_postings = read_jsonl_file_from_s3(bucket_input, file_key)\n",
    "    \n",
    "    # Step 2: Deduplicate (company, title) pairs\n",
    "    unique_pairs = deduplicate_job_postings(job_postings)\n",
    "    \n",
    "    # Step 3: Check cache\n",
    "    cache_hits, cache_misses = check_cache(unique_pairs)\n",
    "    \n",
    "    # Step 4: Call gRPC for cache misses\n",
    "    if cache_misses:\n",
    "        grpc_results = grpc_infer_seniority(cache_misses)\n",
    "        # Step 5: Update cache with gRPC results\n",
    "        update_cache(grpc_results)\n",
    "    else:\n",
    "        grpc_results = {}\n",
    "    \n",
    "    # Step 6: Augment job postings with seniority information\n",
    "    augmented_postings = augment_job_postings(job_postings, cache_hits, grpc_results)\n",
    "    \n",
    "    # Step 7: Write augmented data to output S3 bucket\n",
    "    output_file_key = f\"rl-data/job-postings-mod/{file_key}\"\n",
    "    write_to_s3(bucket_output, output_file_key, augmented_postings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "cache_hits = {}\n",
    "cache_hits[(\"company\", \"title\")] = int(45)\n",
    "cache_hits[(\"company2\", \"title2\")] = int(1)\n",
    "\n",
    "grpc_results = {(\"company3\", \"title3\"): 4 for resp in range(1)}\n",
    "    \n",
    "\n",
    "seniority_info = {**cache_hits, **grpc_results}\n",
    "print(seniority_info.get((\"company3\", \"title\"),None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-cache-assessment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
